% Encoding: UTF-8

@Article{8759970,
  author   = {N. {He} and L. {Fang} and S. {Li} and J. {Plaza} and A. {Plaza}},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Skip-Connected Covariance Network for Remote Sensing Scene Classification},
  year     = {2020},
  number   = {5},
  pages    = {1461-1474},
  volume   = {31},
  doi      = {10.1109/TNNLS.2019.2920374},
  keywords = {rank5},
  ranking  = {rank4},
}

@Article{8421052,
  author   = {Y. {Liu} and Y. {Zhong} and Q. {Qin}},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {Scene Classification Based on Multiscale Convolutional Neural Network},
  year     = {2018},
  issn     = {1558-0644},
  month    = {Dec},
  number   = {12},
  pages    = {7109-7121},
  volume   = {56},
  abstract = {With the large amount of high-spatial resolution images now available, scene classification aimed at obtaining high-level semantic concepts has drawn great attention. The convolutional neural networks (CNNs), which are typical deep learning methods, have widely been studied to automatically learn features for the images for scene classification. However, scene classification based on CNNs is still difficult due to the scale variation of the objects in remote sensing imagery. In this paper, a multiscale CNN (MCNN) framework is proposed to solve the problem. In MCNN, a network structure containing dual branches of a fixed-scale net (F-net) and a varied-scale net (V-net) is constructed and the parameters are shared by the F-net and V-net. The images and their rescaled images are fed into the F-net and V-net, respectively, allowing us to simultaneously train the shared network weights on multiscale images. Furthermore, to ensure that the features extracted from MCNN are scale invariant, a similarity measure layer is added to MCNN, which forces the two feature vectors extracted from the image and its corresponding rescaled image to be as close as possible in the training phase. To demonstrate the effectiveness of the proposed method, we compared the results obtained using three widely used remote sensing data sets: the UC Merced data set, the aerial image data set, and the google data set of SIRI-WHU. The results confirm that the proposed method performs significantly better than the other state-of-the-art scene classification methods.},
  doi      = {10.1109/TGRS.2018.2848473},
  keywords = {convolution;feature extraction;feedforward neural nets;geophysical image processing;image classification;learning (artificial intelligence);remote sensing;multiscale convolutional neural network;high-spatial resolution images;high-level semantic concepts;CNNs;typical deep learning methods;scale variation;multiscale CNN framework;MCNN;network structure;fixed-scale net;varied-scale net;V-net;F-net;shared network weights;multiscale images;aerial image data;remote sensing data sets;scene classification methods;remote sensing imagery;images rescaling;SIRI-WHU;Feature extraction;Remote sensing;Semantics;Convolutional neural networks;Training data;Machine learning;Object oriented modeling;Convolutional neural networks (CNNs);multiscale;scene classification;similarity measure},
}

@InProceedings{9073614,
  author    = {S. A. {Fatima} and A. {Kumar} and A. {Pratap} and S. S. {Raoof}},
  booktitle = {2020 International Conference on Artificial Intelligence and Signal Processing (AISP)},
  title     = {Object Recognition and Detection in Remote Sensing Images: A Comparative Study},
  year      = {2020},
  month     = {Jan},
  pages     = {1-5},
  abstract  = {In this paper, we have provided a brief literature survey of object recognition & detection in remote sensing. These scene recognition images have more components and more challenging issues in the range of aerial image resolutions. It also shows a critical act as a limited area based on functions. The object detection in optical remote sensing has the best critical part of over supposing conditions of characteristic against affecting a model inputs. Certain moving characteristic act as a set of quality to define a based on personal action. This paper provides a brief summary of different object detection in remote sensing images and also discuss about their strength and limitation. The main focus of this review paper is on the satellite image. A subsists in patent of scene recognition of current model act on a freshly process. Inter relates to the land uses analysis system, other model analysis process as well as turn to the appropriate function. We have also discussed the problems and advancement of current scenario and give three research directions for deep learning for medical image recognition, image classification, and health care. We ensure this review article will provide adequate directions and scope for the betterment of the research community in the field of object recognition & detection in remote sensing.},
  doi       = {10.1109/AISP48273.2020.9073614},
  groups    = {surveys},
  issn      = {2640-5768},
  keywords  = {geophysical image processing;health care;image classification;image resolution;learning (artificial intelligence);object detection;object recognition;remote sensing;remote sensing images;object recognition;scene recognition images;aerial image resolutions;optical remote sensing;object detection;satellite image;model analysis process;medical image recognition;image classification;deep learning;health care;Object detection;Machine learning;Remote sensing;Analytical models;Image recognition;Biomedical imaging;Object recognition;Object Recognition;Object Detection;Remote Sensing;Scene Recognition;Deep learning},
}

@InProceedings{10.1145/1869790.1869829,
  author    = {Yang, Yi and Newsam, Shawn},
  booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  title     = {Bag-of-Visual-Words and Spatial Extensions for Land-Use Classification},
  year      = {2010},
  address   = {New York, NY, USA},
  note      = {UC-Merced Datasets},
  pages     = {270–279},
  publisher = {Association for Computing Machinery},
  series    = {GIS '10},
  abstract  = {We investigate bag-of-visual-words (BOVW) approaches to land-use classification in high-resolution overhead imagery. We consider a standard non-spatial representation in which the frequencies but not the locations of quantized image features are used to discriminate between classes analogous to how words are used for text document classification without regard to their order of occurrence. We also consider two spatial extensions, the established spatial pyramid match kernel which considers the absolute spatial arrangement of the image features, as well as a novel method which we term the spatial co-occurrence kernel that considers the relative arrangement. These extensions are motivated by the importance of spatial structure in geographic data.The methods are evaluated using a large ground truth image dataset of 21 land-use classes. In addition to comparisons with standard approaches, we perform extensive evaluation of different configurations such as the size of the visual dictionaries used to derive the BOVW representations and the scale at which the spatial relationships are considered.We show that even though BOVW approaches do not necessarily perform better than the best standard approaches overall, they represent a robust alternative that is more effective for certain land-use classes. We also show that extending the BOVW approach with our proposed spatial co-occurrence kernel consistently improves performance.},
  doi       = {10.1145/1869790.1869829},
  isbn      = {9781450304283},
  keywords  = {land-use classification, local invariant features, bag-of-visual-words},
  location  = {San Jose, California},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1869790.1869829},
}

@Article{7907303,
  author   = {G. {Xia} and J. {Hu} and F. {Hu} and B. {Shi} and X. {Bai} and Y. {Zhong} and L. {Zhang} and X. {Lu}},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification},
  year     = {2017},
  issn     = {1558-0644},
  month    = {July},
  number   = {7},
  pages    = {3965-3981},
  volume   = {55},
  abstract = {Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in the remote sensing area, and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing data sets for aerial scene classification, such as UC-Merced data set and WHU-RS19, contain relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image data set (AID): a large-scale data set for aerial scene classification. The goal of AID is to advance the state of the arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than 10000 aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark.},
  doi      = {10.1109/TGRS.2017.2685945},
  groups   = {surveys},
  keywords = {antennas;geophysical image processing;image classification;learning (artificial intelligence);remote sensing;aerial scene classification techniques;semantic category;high-resolution remote sensing imagery;scene classification algorithms;machine learning approach;data-driven approach;UC-Merced data set;WHU-RS19;aerial image data set;AID;deep learning methods;Remote sensing;Benchmark testing;Earth;Google;Semantics;Rivers;Performance evaluation;Aerial images;benchmark;scene classification},
}

@Article{7891544,
  author   = {G. {Cheng} and J. {Han} and X. {Lu}},
  journal  = {Proceedings of the IEEE},
  title    = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},
  year     = {2017},
  issn     = {1558-2256},
  month    = {Oct},
  number   = {10},
  pages    = {1865-1883},
  volume   = {105},
  abstract = {Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed “NWPU-RESISC45,” which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.},
  doi      = {10.1109/JPROC.2017.2675998},
  groups   = {surveys},
  keywords = {geophysical image processing;learning (artificial intelligence);remote sensing;remote sensing image scene classification;various data sets;scene classification;data sets;image numbers;image variations;image diversity;learning based methods;NWPU-RESISC45;RESISC;Northwestern Polytechnical University;NWPU;representative methods;data-driven algorithms;Remote sensing;Benchmark testing;Spatial resolution;Social network services;Satellites;Image analysis;Machine learning;Unsupervised learning;Classification;Benchmark data set;deep learning;handcrafted features;remote sensing image;scene classification;unsupervised feature learning},
}

@Article{9127795,
  author   = {G. {Cheng} and X. {Xie} and J. {Han} and L. {Guo} and G. -S. {Xia}},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  title    = {Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities},
  year     = {2020},
  issn     = {2151-1535},
  pages    = {3735-3756},
  volume   = {13},
  abstract = {Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this article provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey: first, autoencoder-based remote sensing image scene classification methods; second, convolutional neural network-based remote sensing image scene classification methods; and third, generative adversarial network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly used benchmark datasets. Finally, we discuss the promising opportunities for further research.},
  doi      = {10.1109/JSTARS.2020.3005403},
  groups   = {surveys},
  keywords = {convolutional neural nets;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);remote sensing;generative adversarial network-based remote sensing image scene classification methods;convolutional neural network-based remote sensing image scene classification methods;autoencoder-based remote sensing image scene classification methods;deep learning;Remote sensing;Deep learning;Semantics;Earth;Benchmark testing;Image analysis;Sensors;Deep learning;remote sensing image;scene classification},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:CompactModels\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:surveys\;0\;1\;0x8a8a8aff\;\;\;;
}
