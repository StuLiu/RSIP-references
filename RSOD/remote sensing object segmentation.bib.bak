% Encoding: UTF-8

@Article{LI2020296,
  author     = {Ke Li and Gang Wan and Gong Cheng and Liqiu Meng and Junwei Han},
  journal    = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title      = {Object detection in optical remote sensing images: A survey and a new benchmark},
  year       = {2020},
  issn       = {0924-2716},
  pages      = {296-307},
  volume     = {159},
  abstract   = {Substantial efforts have been devoted more recently to presenting various methods for object detection in optical remote sensing images. However, the current survey of datasets and deep learning based methods for object detection in optical remote sensing images is not adequate. Moreover, most of the existing datasets have some shortcomings, for example, the numbers of images and object categories are small scale, and the image diversity and variations are insufficient. These limitations greatly affect the development of deep learning based object detection methods. In the paper, we provide a comprehensive review of the recent deep learning based object detection progress in both the computer vision and earth observation communities. Then, we propose a large-scale, publicly available benchmark for object DetectIon in Optical Remote sensing images, which we name as DIOR. The dataset contains 23,463 images and 192,472 instances, covering 20 object classes. The proposed DIOR dataset (1) is large-scale on the object categories, on the object instance number, and on the total image number; (2) has a large range of object size variations, not only in terms of spatial resolutions, but also in the aspect of inter- and intra-class size variability across objects; (3) holds big variations as the images are obtained with different imaging conditions, weathers, seasons, and image quality; and (4) has high inter-class similarity and intra-class diversity. The proposed benchmark can help the researchers to develop and validate their data-driven methods. Finally, we evaluate several state-of-the-art approaches on our DIOR dataset to establish a baseline for future research.},
  doi        = {10.1016/j.isprsjprs.2019.11.023},
  groups     = {surveys},
  keywords   = {Object detection, Deep learning, Convolutional Neural Network (CNN), Benchmark dataset, Optical remote sensing images},
  readstatus = {read},
  url        = {https://www.sciencedirect.com/science/article/pii/S0924271619302825},
}

@Article{CHENG201611,
  author   = {Gong Cheng and Junwei Han},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title    = {A survey on object detection in optical remote sensing images},
  year     = {2016},
  issn     = {0924-2716},
  pages    = {11-28},
  volume   = {117},
  abstract = {Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey (1) template matching-based object detection methods, (2) knowledge-based object detection methods, (3) object-based image analysis (OBIA)-based object detection methods, (4) machine learning-based object detection methods, and (5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field.},
  doi      = {https://doi.org/10.1016/j.isprsjprs.2016.03.014},
  groups   = {surveys},
  keywords = {Object detection, Optical remote sensing images, Template matching, Object-based image analysis (OBIA), Machine learning, Deep learning, Weakly supervised learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0924271616300144},
}

@InProceedings{6909475,
  author     = {R. {Girshick} and J. {Donahue} and T. {Darrell} and J. {Malik}},
  booktitle  = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title      = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  year       = {2014},
  month      = {June},
  note       = {R-CNN},
  pages      = {580-587},
  abstract   = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  doi        = {10.1109/CVPR.2014.81},
  issn       = {1063-6919},
  keywords   = {image segmentation;neural nets;object detection;rich feature hierarchy;semantic segmentation;object detection performance;canonical PASCAL VOC dataset;low-level image feature;detection algorithm;mean average precision;mAP;high-capacity convolutional neural network;bottom-up region proposal;segment objects;labeled training data;supervised pretraining;auxiliary task;domain-specific fine-tuning;performance boost;R-CNN;image features;source code;Proposals;Feature extraction;Training;Visualization;Object detection;Vectors;Support vector machines},
  readstatus = {skimmed},
}

@Article{7005506,
  author   = {K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
  year     = {2015},
  issn     = {1939-3539},
  month    = {Sep.},
  note     = {SPP},
  number   = {9},
  pages    = {1904-1916},
  volume   = {37},
  abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224$\times$ 224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 $\times$  faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank } # 2 in object detection and  #{3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
  doi      = {10.1109/TPAMI.2015.2389824},
  keywords = {Training;Feature extraction;Accuracy;Convolutional codes;Agriculture;Testing;Vectors;Convolutional Neural Networks;Spatial Pyramid Pooling;Image Classification;Object Detection;Convolutional neural networks;spatial pyramid pooling;image classification;object detection;Algorithms;Animals;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pattern Recognition, Automated},
}

@InProceedings{7410526,
  author    = {R. {Girshick}},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Fast R-CNN},
  year      = {2015},
  month     = {Dec},
  note      = {Fast R-CNN},
  pages     = {1440-1448},
  abstract  = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  doi       = {10.1109/ICCV.2015.169},
  issn      = {2380-7504},
  keywords  = {feedforward neural nets;object detection;fast R-CNN;fast region-based convolutional network method;object detection;VGG16 network;Python;C++;Caffe;open-source MIT License;Training;Proposals;Feature extraction;Object detection;Pipelines;Computer architecture;Open source software},
}

@Article{7485869,
  author   = {S. {Ren} and K. {He} and R. {Girshick} and J. {Sun}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  year     = {2017},
  issn     = {1939-3539},
  month    = {June},
  note     = {Faster R-CNN},
  number   = {6},
  pages    = {1137-1149},
  volume   = {39},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network(RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  doi      = {10.1109/TPAMI.2016.2577031},
  keywords = {graphics processing units;neural nets;object detection;faster-R-CNN;real-time object detection;region proposal networks;RPN;full-image convolutional features;high-quality region proposals;attention mechanisms;deep VGG-16 model;GPU;object detection accuracy;PASCAL VOC 2007;PASCAL VOC 2012;MS COCO datasets;COCO 2015 competitions;ILSVRC;Proposals;Object detection;Convolutional codes;Feature extraction;Search problems;Detectors;Training;Object detection;region proposal;convolutional neural network},
}

@Article{OverFeat2014,
  author  = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and Lecun, Yann},
  journal = {International Conference on Learning Representations (ICLR)},
  title   = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
  year    = {2014},
  month   = {12},
  note    = {OverFeat},
  url     = {https://arxiv.org/abs/1312.6229},
}

@InProceedings{7780460,
  author    = {J. {Redmon} and S. {Divvala} and R. {Girshick} and A. {Farhadi}},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  year      = {2016},
  month     = {June},
  note      = {YOLOv1},
  pages     = {779-788},
  abstract  = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  doi       = {10.1109/CVPR.2016.91},
  issn      = {1063-6919},
  keywords  = {image classification;image representation;neural nets;object detection;you only look once;unified real-time object detection;object classifiers;bounding boxes;class probabilities;neural network;detection pipeline;detection performance;YOLO model;object representation;DPM;R-CNN;natural images;Computer architecture;Microprocessors;Object detection;Training;Real-time systems;Neural networks;Pipelines},
}

@InProceedings{8100173,
  author    = {J. {Redmon} and A. {Farhadi}},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {YOLO9000: Better, Faster, Stronger},
  year      = {2017},
  month     = {July},
  note      = {YOLOv2},
  pages     = {6517-6525},
  abstract  = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
  doi       = {10.1109/CVPR.2017.690},
  issn      = {1063-6919},
  keywords  = {image classification;object detection;YOLO9000;COCO detection dataset;ImageNet detection task;YOLO detection method;YOLOv2 model;object detection system;PASCAL VOC;object classification;ImageNet classification dataset;Image resolution;Feature extraction;Training;Real-time systems;Object detection;Detectors},
}

@Article{2018arXiv180402767R,
  author        = {{Redmon}, Joseph and {Farhadi}, Ali},
  journal       = {arXiv e-prints},
  title         = {{YOLOv3: An Incremental Improvement}},
  year          = {2018},
  month         = apr,
  note          = {YOLOv3},
  pages         = {arXiv:1804.02767},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180402767R},
  archiveprefix = {arXiv},
  eid           = {arXiv:1804.02767},
  eprint        = {1804.02767},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
}

@InProceedings{10.1007/978-3-319-46448-0_2,
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  booktitle = {Computer Vision -- ECCV 2016},
  title     = {SSD: Single Shot MultiBox Detector},
  year      = {2016},
  address   = {Cham},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  note      = {SSD},
  pages     = {21--37},
  publisher = {Springer International Publishing},
  abstract  = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\$}{\$}300 {\backslash}times 300{\$}{\$}300{\texttimes}300input, SSD achieves 74.3 {\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\$}{\$}512 {\backslash}times 512{\$}{\$}512{\texttimes}512input, SSD achieves 76.9 {\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  isbn      = {978-3-319-46448-0},
}

@InProceedings{9010985,
  author    = {K. {Duan} and S. {Bai} and L. {Xie} and H. {Qi} and Q. {Huang} and Q. {Tian}},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {CenterNet: Keypoint Triplets for Object Detection},
  year      = {2019},
  month     = {Oct},
  note      = {CenterNet},
  pages     = {6568-6577},
  abstract  = {In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet.},
  doi       = {10.1109/ICCV.2019.00667},
  issn      = {2380-7504},
  keywords  = {neural nets;object detection;MS-COCO dataset;representative one-stage keypoint-based detector;CenterNet;minimal costs;visual patterns;cropped regions;incorrect object bounding boxes;keypoint-based approaches;object detection;keypoint triplets;two-stage detectors;central regions;center pooling;cascade corner pooling;Object detection;Heating systems;Visualization;Detectors;Proposals;Feature extraction;Task analysis},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:surveys\;0\;0\;0x8a8a8aff\;\;\;;
}
